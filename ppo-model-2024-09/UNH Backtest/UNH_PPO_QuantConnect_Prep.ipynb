{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/quant-trading-model-zoo/blob/main/PPO_QuantConnect_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFYht8BLBDv5"
      },
      "outputs": [],
      "source": [
        "!pip install \"shimmy>=2.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xMJutMCD_uu"
      },
      "outputs": [],
      "source": [
        "!pip -q install yfinance pywavelets transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILVF8lQ7ibyY"
      },
      "outputs": [],
      "source": [
        "!apt-get remove --purge -y cuda* libcuda* nvidia* || echo \"No conflicting CUDA packages\"\n",
        "!apt-get autoremove -y\n",
        "!apt-get clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss1DpUiHinrL"
      },
      "outputs": [],
      "source": [
        "!apt-get update -qq && apt-get install -y \\\n",
        "    libcusolver11 libcusparse11 libcurand10 libcufft10 libnppig10 libnppc10 libnppial10 \\\n",
        "    cuda-toolkit-12-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-3wUYhMjK-s"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.20.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeXuomvZjPD6"
      },
      "outputs": [],
      "source": [
        "!pip install --extra-index-url=https://pypi.nvidia.com \\\n",
        "    cuml-cu12==25.2.0 cudf-cu12==25.2.0 cupy-cuda12x \\\n",
        "    dask-cuda==25.2.0 dask-cudf-cu12==25.2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_tMI99TjTni"
      },
      "outputs": [],
      "source": [
        "!pip install numba==0.60.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kHVvDWyvwdt"
      },
      "outputs": [],
      "source": [
        "!pip install \"stable-baselines3[extra]>=2.0.0\" \"gymnasium>=0.29\" \"shimmy>=2.0.0\" \\\n",
        "  gym-anytrading yfinance pandas numpy scikit-learn xgboost joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cFCJkVhjYcj"
      },
      "outputs": [],
      "source": [
        "#!pip install stable-baselines3[extra] gymnasium gym-anytrading yfinance xgboost joblib\n",
        "#!pip install matplotlib scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDqANXjRjbY8"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTQMOopCjd_Y"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xlm1PrOjgys"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"TensorFlow GPU memory growth enabled\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"TensorFlow GPU memory config failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gUz7N8ajjqs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda-12.4'\n",
        "os.environ['PATH'] += ':/usr/local/cuda-12.4/bin'\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/local/cuda-12.4/lib64'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxTo_GOSemtU"
      },
      "outputs": [],
      "source": [
        "#Step 7: authenticate with hugging face hub (optional)\n",
        "#This allows for better access and avoids rate limits when downloading public models/datasets\n",
        "\n",
        "# Authenticate with Hugging Face Hub\n",
        "#notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Upload your local env file (e.g., Alpaca_keys.env.txt or .env.github)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()   # run this cell and choose the file"
      ],
      "metadata": {
        "id": "fK_jkdQlHB3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2) Rename to .env (only if your filename isn't already \".env\")\n",
        "import os\n",
        "if \"Alpaca_keys.env.txt\" in uploaded:\n",
        "    os.rename(\"Alpaca_keys.env.txt\", \".env\")    # adjust if your uploaded name differs\n"
      ],
      "metadata": {
        "id": "5We3sEjgpqhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Load variables from .env without printing them\n",
        "!pip -q install python-dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\".env\")  # or your actual filename\n",
        "\n",
        "import os\n",
        "token = os.getenv(\"GITHUB_TOKEN\")      # ← this reads from the .env\n",
        "print(\"Token present?\", bool(token))   # Do NOT print the token itself\n"
      ],
      "metadata": {
        "id": "t-MxocSjHIiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, subprocess, os, json, time, warnings, logging\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# ---------------- Install dependencies (idempotent) ----------------\n",
        "def _pip(pkgs: List[str]):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *pkgs])\n",
        "\n",
        "try:\n",
        "    import stable_baselines3, gymnasium, yfinance, requests, gym_anytrading, torch, numpy, pandas  # noqa: F401\n",
        "except Exception:\n",
        "    _pip([\n",
        "        \"torch==2.2.2\",\n",
        "        \"stable-baselines3==2.2.1\",\n",
        "        \"gymnasium==0.29.1\",\n",
        "        \"gym-anytrading==1.3.4\",\n",
        "        \"numpy==1.26.4\",\n",
        "        \"pandas==2.2.2\",\n",
        "        \"yfinance==0.2.40\",\n",
        "        \"requests==2.32.3\"\n",
        "    ])\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import yfinance as yf\n",
        "import gymnasium as gym\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "from gymnasium.spaces import Box as GBox\n",
        "import torch\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "# ---------------- Logging ----------------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "L = logging.getLogger(\"producer\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------------- Google Drive mount ----------------\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    if not os.path.ismount(\"/content/drive\"):\n",
        "        drive.mount(\"/content/drive\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "RESULTS_ROOT   = \"/content/drive/MyDrive/Results_May_2025\"\n",
        "MASTER_DIR     = os.path.join(RESULTS_ROOT, \"ppo_models_master\")  # where <prefix>_model.zip etc. live\n",
        "\n",
        "# (Symbol -> prefix) from your saved models\n",
        "PICKS: Dict[str, str] = {\n",
        "    \"UNH\":  \"ppo_UNH_window3\",\n",
        "    \"TSLA\": \"ppo_TSLA_window2\",\n",
        "    \"TMO\":  \"ppo_TMO_window3\",\n",
        "}\n",
        "\n",
        "# yfinance fetch\n",
        "YF_INTERVAL = \"1m\"\n",
        "YF_DAYS     = 5\n",
        "\n",
        "# Gist settings (token must be set by you in the session)\n",
        "GITHUB_TOKEN  = os.environ.get(\"GITHUB_TOKEN\", \"\").strip()\n",
        "GIST_ID       = os.environ.get(\"GIST_ID\", \"\").strip()   # leave empty on first run; we persist to file below\n",
        "STATE_DIR     = \"/content/drive/MyDrive/QuantConnect_Ready\"\n",
        "os.makedirs(STATE_DIR, exist_ok=True)\n",
        "GIST_ID_PATH  = os.path.join(STATE_DIR, \"live_signals_gist_id.txt\")\n",
        "GIST_FILENAME = \"live_signals.json\"\n",
        "GIST_DESC     = \"Live PPO signals for QC (Producer→Consumer)\"\n",
        "\n",
        "# Publish loop\n",
        "RUN_LOOP  = False\n",
        "SLEEP_SEC = 60\n",
        "\n",
        "# ---------------- Feature function (fallback; replace with your real pipeline if desired) ----------------\n",
        "try:\n",
        "    compute_enhanced_features  # type: ignore\n",
        "except NameError:\n",
        "    def compute_enhanced_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "        # No-op fallback. Keep column names as standard OHLCV + whatever yfinance returns.\n",
        "        # If your features.json expects extra engineered columns, create them here.\n",
        "        return df_in\n",
        "\n",
        "# ---------------- PPO Env (must match training time semantics) ----------------\n",
        "class ContinuousPositionEnv(StocksEnv):\n",
        "    def __init__(self, df, frame_bound, window_size,\n",
        "                 cost_rate=0.0002, slip_rate=0.0003,\n",
        "                 k_alpha=0.20, k_mom=0.05, k_sent=0.0,\n",
        "                 mom_source=\"denoised\", mom_lookback=20,\n",
        "                 min_trade_delta=0.01, cooldown=5, reward_clip=1.0):\n",
        "        super().__init__(df=df.reset_index(drop=True), frame_bound=frame_bound, window_size=window_size)\n",
        "        if isinstance(self.observation_space, gym.spaces.Box):\n",
        "            self.observation_space = GBox(\n",
        "                low=self.observation_space.low,\n",
        "                high=self.observation_space.high,\n",
        "                shape=self.observation_space.shape,\n",
        "                dtype=self.observation_space.dtype,\n",
        "            )\n",
        "        self.action_space = GBox(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "        self.cost_rate, self.slip_rate = float(cost_rate), float(slip_rate)\n",
        "        self.k_alpha, self.k_mom = float(k_alpha), float(k_mom)\n",
        "        self.k_sent = float(k_sent)\n",
        "        self.mom_source, self.mom_lookback = str(mom_source), int(mom_lookback)\n",
        "        self.min_trade_delta, self.cooldown = float(min_trade_delta), int(cooldown)\n",
        "        self.reward_clip = float(reward_clip)\n",
        "        self.nav, self.pos, self._last_trade_step = 1.0, 0.0, -self.cooldown\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        out = super().reset(**kwargs)\n",
        "        if isinstance(out, tuple): obs, info = out\n",
        "        else: obs, info = out, {}\n",
        "        self.nav, self.pos, self._last_trade_step = 1.0, 0.0, -self.cooldown\n",
        "        info = info or {}\n",
        "        info.update({\"nav\": self.nav, \"pos\": self.pos})\n",
        "        return obs, info\n",
        "\n",
        "    def _step_parent_hold(self):\n",
        "        step_result = super().step(2)\n",
        "        if len(step_result) == 5:\n",
        "            obs, _env_rew, terminated, truncated, info = step_result\n",
        "        else:\n",
        "            obs, _env_rew, done, info = step_result\n",
        "            terminated, truncated = bool(done), False\n",
        "        return obs, terminated, truncated, info\n",
        "\n",
        "    def _ret_t(self):\n",
        "        cur  = float(self.df.loc[self._current_tick, 'Close'])\n",
        "        prev = float(self.df.loc[max(self._current_tick - 1, 0), 'Close'])\n",
        "        return 0.0 if prev <= 0 else (cur - prev) / prev\n",
        "\n",
        "    def _mom_signal(self):\n",
        "        if self.mom_source == \"macd\" and \"MACD_Line\" in self.df.columns:\n",
        "            recent = self.df[\"MACD_Line\"].iloc[max(self._current_tick-200,0):self._current_tick+1]\n",
        "            return float(np.tanh(float(self.df.loc[self._current_tick, \"MACD_Line\"]) / (1e-6 + recent.std())))\n",
        "        if \"Denoised_Close\" in self.df.columns and self._current_tick - self.mom_lookback >= 0:\n",
        "            now  = float(self.df.loc[self._current_tick, \"Denoised_Close\"])\n",
        "            then = float(self.df.loc[self._current_tick - self.mom_lookback, \"Denoised_Close\"])\n",
        "            base = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "            slope = (now - then) / max(self.mom_lookback, 1)\n",
        "            return float(np.tanh(10.0 * (slope / max(abs(base), 1e-6))))\n",
        "        return 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        a = float(np.array(action).squeeze()); target_pos = float(np.clip(a, -1.0, 1.0))\n",
        "        r_t = self._ret_t(); base_ret = self.pos * r_t\n",
        "        changed = (abs(target_pos - self.pos) >= 0.01) and ((self._current_tick - self._last_trade_step) >= 5)\n",
        "        delta_pos = (target_pos - self.pos) if changed else 0.0\n",
        "        trade_cost = (0.0002 + 0.0003) * abs(delta_pos)\n",
        "        rel_alpha  = base_ret - r_t\n",
        "        mom_term   = self.pos * self._mom_signal()\n",
        "        shaped     = base_ret + 0.20*rel_alpha + 0.05*mom_term - trade_cost\n",
        "        reward     = float(np.clip(shaped, -1.0, 1.0))\n",
        "        self.nav  *= (1.0 + base_ret - trade_cost)\n",
        "        if changed:\n",
        "            self.pos = target_pos; self._last_trade_step = self._current_tick\n",
        "        obs, terminated, truncated, info = self._step_parent_hold()\n",
        "        info = info or {}\n",
        "        info.update({\"ret_t\": r_t, \"nav\": self.nav, \"pos\": self.pos,\n",
        "                     \"trade_cost\": trade_cost, \"base_ret\": base_ret,\n",
        "                     \"rel_alpha\": rel_alpha, \"mom\": self._mom_signal()})\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "# ---------------- Helpers: artifacts, features, model load ----------------\n",
        "def _features_list_for(prefix: str) -> List[str]:\n",
        "    fpath = os.path.join(MASTER_DIR, f\"{prefix}_features.json\")\n",
        "    if os.path.exists(fpath):\n",
        "        try:\n",
        "            meta = json.load(open(fpath, \"r\"))\n",
        "            feats = meta.get(\"features\") or []\n",
        "            if isinstance(feats, list):\n",
        "                return [str(c) for c in feats]\n",
        "        except Exception as e:\n",
        "            L.warning(f\"features.json read failed for {prefix}: {e}\")\n",
        "    return []\n",
        "\n",
        "def _align_columns(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
        "    feats = _features_list_for(prefix)\n",
        "    if not feats:\n",
        "        return df\n",
        "    aligned = df.copy()\n",
        "    for c in feats:\n",
        "        if c not in aligned.columns:\n",
        "            aligned[c] = 0.0\n",
        "    # Optional: order features first, then anything extra\n",
        "    ordered = [c for c in feats if c in aligned.columns] + [c for c in aligned.columns if c not in feats]\n",
        "    return aligned[ordered]\n",
        "\n",
        "def _check_artifacts(prefix: str) -> Dict[str, bool]:\n",
        "    need = [\"_model.zip\", \"_vecnorm.pkl\", \"_features.json\", \"_probability_config.json\", \"_model_info.json\"]\n",
        "    return {s: os.path.exists(os.path.join(MASTER_DIR, prefix + s)) for s in need}\n",
        "\n",
        "def get_mu_sigma(model, obs):\n",
        "    with torch.no_grad():\n",
        "        obs_t, _     = model.policy.obs_to_tensor(obs)\n",
        "        feats        = model.policy.extract_features(obs_t)\n",
        "        latent_pi, _ = model.policy.mlp_extractor(feats)\n",
        "        mean_actions = model.policy.action_net(latent_pi)\n",
        "        log_std      = model.policy.log_std\n",
        "        mu    = float(mean_actions.detach().cpu().numpy().squeeze())\n",
        "        sigma = float(log_std.exp().detach().cpu().numpy().squeeze())\n",
        "    return mu, sigma\n",
        "\n",
        "def load_model_and_env(prefix: str):\n",
        "    model_path = os.path.join(MASTER_DIR, f\"{prefix}_model.zip\")\n",
        "    vec_path   = os.path.join(MASTER_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Missing model: {model_path}\")\n",
        "    model = PPO.load(model_path, device=\"cpu\")\n",
        "\n",
        "    def make_env(df_window: pd.DataFrame):\n",
        "        frame_bound = (50, len(df_window) - 3)\n",
        "        e = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "            df=df_window, frame_bound=frame_bound, window_size=10,\n",
        "            cost_rate=0.0002, slip_rate=0.0003,\n",
        "            k_alpha=0.20, k_mom=0.05, k_sent=0.0,\n",
        "            mom_source=\"denoised\", mom_lookback=20,\n",
        "            min_trade_delta=0.01, cooldown=5, reward_clip=1.0\n",
        "        )])\n",
        "        if os.path.exists(vec_path):\n",
        "            e = VecNormalize.load(vec_path, e)\n",
        "        e.training = False; e.norm_reward = False\n",
        "        return e\n",
        "    return model, make_env\n",
        "\n",
        "# ---------------- Data fetch + MultiIndex fix + predict ----------------\n",
        "def _flatten_yf_columns(df: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    If yfinance returns a MultiIndex (e.g. ('Close','UNH')), flatten to plain columns:\n",
        "      ('Close','UNH') -> 'Close'\n",
        "      ('Adj Close','') -> 'Adj Close'\n",
        "      otherwise -> f\"{lvl0}_{lvl1}\"\n",
        "    \"\"\"\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        new_cols = []\n",
        "        for c0, c1 in df.columns.to_list():\n",
        "            if c1 in (\"\", symbol):\n",
        "                new_cols.append(c0)\n",
        "            else:\n",
        "                new_cols.append(f\"{c0}_{c1}\")\n",
        "        df.columns = new_cols\n",
        "    return df\n",
        "\n",
        "def latest_df_for_symbol(symbol: str, horizon_days: int = YF_DAYS, interval: str = YF_INTERVAL) -> pd.DataFrame | None:\n",
        "    end   = datetime.now(timezone.utc)\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df = yf.download(symbol,\n",
        "                     start=start.strftime(\"%Y-%m-%d\"),\n",
        "                     end=end.strftime(\"%Y-%m-%d\"),\n",
        "                     interval=interval,\n",
        "                     progress=False,\n",
        "                     auto_adjust=False)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "    df = df.reset_index()\n",
        "    df[\"Symbol\"] = symbol\n",
        "    df = _flatten_yf_columns(df, symbol)\n",
        "    df = compute_enhanced_features(df)\n",
        "    return df\n",
        "\n",
        "def predict_latest(symbol: str, prefix: str) -> Dict[str, Any]:\n",
        "    # artifacts present?\n",
        "    missing = [k for k, v in _check_artifacts(prefix).items() if not v]\n",
        "    if missing:\n",
        "        return {\"symbol\": symbol, \"prefix\": prefix, \"error\": f\"missing artifacts: {missing}\"}\n",
        "\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 100:\n",
        "        return {\"symbol\": symbol, \"prefix\": prefix, \"error\": \"no fresh data\"}\n",
        "\n",
        "    # align to training features if provided\n",
        "    live_df = _align_columns(live_df, prefix)\n",
        "\n",
        "    # window\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "\n",
        "    # roll to last bar with HOLD\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs, _ = obs\n",
        "    for _ in range(len(df_window) - 1):\n",
        "        obs, _, dones, _ = env.step([np.array([0.0], dtype=np.float32)])\n",
        "        if isinstance(dones, (np.ndarray, list)) and dones[0]:\n",
        "            break\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    mu, sigma = get_mu_sigma(model, obs)\n",
        "    from math import erf, sqrt\n",
        "    Phi = lambda x: 0.5 * (1.0 + erf(x / sqrt(2.0)))\n",
        "    p_long = 1.0 - Phi((0.0 - mu) / max(sigma, 1e-6))\n",
        "\n",
        "    a = float(np.array(action).squeeze())\n",
        "    signal = \"BUY\" if a > 0.10 else (\"SELL\" if a < -0.30 else \"HOLD\")\n",
        "    ts = df_window[\"Datetime\"].iloc[-1] if \"Datetime\" in df_window.columns else None\n",
        "    px = float(df_window[\"Close\"].iloc[-1])\n",
        "\n",
        "    return {\n",
        "        \"symbol\": symbol,\n",
        "        \"prefix\": prefix,\n",
        "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"bar_ts\": str(ts),\n",
        "        \"price\": px,\n",
        "        \"action\": a,\n",
        "        \"signal\": signal,\n",
        "        \"confidence\": abs(a),\n",
        "        \"p_long\": float(p_long),\n",
        "        \"p_short\": float(1.0 - p_long),\n",
        "        \"mu\": float(mu),\n",
        "        \"sigma\": float(sigma)\n",
        "    }\n",
        "\n",
        "# ---------------- Gist publisher ----------------\n",
        "def _headers(token: str) -> Dict[str, str]:\n",
        "    return {\"Authorization\": f\"token {token}\"} if token else {}\n",
        "\n",
        "def _load_saved_gist_id() -> str:\n",
        "    if GIST_ID:\n",
        "        return GIST_ID\n",
        "    try:\n",
        "        return open(GIST_ID_PATH, \"r\").read().strip()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def _save_gist_id(gid: str):\n",
        "    try:\n",
        "        with open(GIST_ID_PATH, \"w\") as f:\n",
        "            f.write(gid)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def publish_json_to_gist(payload: dict, filename: str, gist_id: str, token: str, desc: str) -> str:\n",
        "    if not token:\n",
        "        raise RuntimeError(\"GITHUB_TOKEN not set. In a cell above, run: os.environ['GITHUB_TOKEN'] = 'ghp_...'\")\n",
        "\n",
        "    files = {filename: {\"content\": json.dumps(payload, indent=2)}}\n",
        "    if gist_id:\n",
        "        r = requests.patch(f\"https://api.github.com/gists/{gist_id}\",\n",
        "                           headers=_headers(token),\n",
        "                           json={\"files\": files, \"description\": desc})\n",
        "        if r.status_code // 100 != 2:\n",
        "            raise RuntimeError(f\"Gist update failed: {r.status_code} {r.text}\")\n",
        "        return gist_id\n",
        "    else:\n",
        "        r = requests.post(\"https://api.github.com/gists\",\n",
        "                          headers=_headers(token),\n",
        "                          json={\"files\": files, \"description\": desc, \"public\": True})\n",
        "        if r.status_code // 100 != 2:\n",
        "            raise RuntimeError(f\"Gist create failed: {r.status_code} {r.text}\")\n",
        "        new_id = r.json().get(\"id\", \"\")\n",
        "        if new_id:\n",
        "            _save_gist_id(new_id)\n",
        "        return new_id\n",
        "\n",
        "def gist_raw_url(gist_id: str, filename: str) -> str:\n",
        "    meta = requests.get(f\"https://api.github.com/gists/{gist_id}\").json()\n",
        "    owner = (meta.get(\"owner\") or {}).get(\"login\", \"anonymous\")\n",
        "    return f\"https://gist.githubusercontent.com/{owner}/{gist_id}/raw/{filename}\"\n",
        "\n",
        "# ---------------- Run once (or loop) ----------------\n",
        "def run_once():\n",
        "    results = []\n",
        "    for sym, pref in PICKS.items():\n",
        "        try:\n",
        "            out = predict_latest(sym, pref)\n",
        "            if out.get(\"error\"):\n",
        "                L.warning(f\"{sym} -> {out['error']}\")\n",
        "            results.append(out)\n",
        "        except Exception as e:\n",
        "            L.exception(f\"{sym} predict error: {e}\")\n",
        "            results.append({\"symbol\": sym, \"prefix\": pref, \"error\": str(e)})\n",
        "\n",
        "    payload = {\n",
        "        \"generated_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"valid_until_utc\": (datetime.now(timezone.utc) + timedelta(minutes=3)).isoformat(),\n",
        "        \"producer\": \"colab-sb3\",\n",
        "        \"interval\": YF_INTERVAL,\n",
        "        \"models\": results\n",
        "    }\n",
        "\n",
        "    gid = _load_saved_gist_id()\n",
        "    gid = publish_json_to_gist(payload, filename=GIST_FILENAME, gist_id=gid, token=GITHUB_TOKEN, desc=GIST_DESC)\n",
        "    raw = gist_raw_url(gid, GIST_FILENAME)\n",
        "    print(\"Published:\", f\"https://gist.github.com/{gid}\")\n",
        "    print(\"RAW URL  :\", raw)\n",
        "    print(\"Preview  :\", json.dumps(payload, indent=2)[:900], \"...\")\n",
        "\n",
        "# ---------------- Sanity check + execute ----------------\n",
        "for sym, pref in PICKS.items():\n",
        "    status = _check_artifacts(pref)\n",
        "    missing = [k for k, v in status.items() if not v]\n",
        "    print(f\"{pref}: {'OK' if not missing else 'MISSING ' + str(missing)}\")\n",
        "\n",
        "# Run once (set RUN_LOOP=True above for periodic publishing)\n",
        "run_once()\n",
        "if RUN_LOOP:\n",
        "    while True:\n",
        "        try:\n",
        "            run_once()\n",
        "        except Exception as e:\n",
        "            L.error(f\"Publish error: {e}\")\n",
        "        time.sleep(SLEEP_SEC)\n"
      ],
      "metadata": {
        "id": "6cMq8kbmgzts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, requests\n",
        "\n",
        "RESULTS_ROOT     = \"/content/drive/MyDrive/Results_May_2025/ppo_models_master/live_signals_gist_id.txt\"\n",
        "FINAL_MODEL_DIR  = os.path.join(RESULTS_ROOT, \"ppo_models_master\")\n",
        "GIST_ID_PATH     = os.path.join(FINAL_MODEL_DIR, \"live_signals_gist_id.txt\")\n",
        "FILENAME         = \"live_signals.json\"\n",
        "\n",
        "# 1) Get the Gist ID we saved earlier\n",
        "with open(GIST_ID_PATH, \"r\") as f:\n",
        "    gid = f.read().strip()\n",
        "\n",
        "# 2) Fetch Gist metadata (no auth needed for public gists)\n",
        "meta = requests.get(f\"https://api.github.com/gists/{gid}\").json()\n",
        "owner = (meta.get(\"owner\") or {}).get(\"login\", \"anonymous\")\n",
        "files = meta.get(\"files\", {})\n",
        "raw_url_api = files.get(FILENAME, {}).get(\"raw_url\", \"\")\n",
        "\n",
        "# 3) Build a stable RAW URL that doesn’t include a commit hash\n",
        "raw_url_stable = f\"https://gist.githubusercontent.com/{owner}/{gid}/raw/{FILENAME}\"\n",
        "\n",
        "print(\"Gist page:\", f\"https://gist.github.com/{owner}/{gid}\")\n",
        "print(\"Raw (stable):\", raw_url_stable)\n",
        "print(\"Raw (API-provided, may include a revision hash):\", raw_url_api)\n"
      ],
      "metadata": {
        "id": "mPG7buMd3axD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
