{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/racoope70/quant-trading-model-zoo/blob/main/PPO_QuantConnect_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run to get paramaters for QuantConnect (from this codes output)\n",
        "#get live_signals.json → the output your backtest consumes."
      ],
      "metadata": {
        "id": "EtHqFF5L3EKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y opencv-python opencv-python-headless opencv-contrib-python"
      ],
      "metadata": {
        "id": "hSXunPFBywlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall stuff that conflicts with a PyTorch+SB3+Gymnasium workflow (safe if absent)\n",
        "!pip -q uninstall -y \\\n",
        "  gym gymnasium shimmy stable-baselines3 dopamine-rl \\\n",
        "  tensorflow tensorflow-hub tf-keras tensorflow-text tensorflow-decision-forests \\\n",
        "  cudf-cu12 dask-cudf-cu12 dask-cuda rapids-dask-dependency \\\n",
        "  libcudf-cu12 libcuml-cu12 pylibcudf-cu12 pylibraft-cu12 \\\n",
        "  libcugraph-cu12 pylibcugraph-cu12 rmm-cu12 libcuvs-cu12 cuvs-cu12 \\\n",
        "  cupy-cuda12x opencv-python opencv-python-headless opencv-contrib-python || true\n",
        "\n",
        "# PyTorch (CUDA 12.4 wheels that match Colab GPU VMs)\n",
        "!pip -q install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio\n",
        "\n",
        "# Core RL/ML stack pinned to avoid Colab’s resolver nags\n",
        "# - pandas==2.2.2 and requests==2.32.4 match google-colab’s constraints\n",
        "# - numpy==2.0.2 avoids the OpenCV / numba complaints and works fine with SB3\n",
        "!pip -q install -U \\\n",
        "  \"protobuf>=5.29.1,<6\" \\\n",
        "  \"gymnasium>=1.1,<1.3\" \\\n",
        "  \"stable-baselines3==2.7.0\" \\\n",
        "  \"numpy==2.0.2\" \"pandas==2.2.2\" \"requests==2.32.4\" \\\n",
        "  yfinance pywavelets transformers python-dotenv\n"
      ],
      "metadata": {
        "id": "DtTdPuFmmfuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shell cell\n",
        "!pip -q install \"opencv-python-headless==4.12.0.88\""
      ],
      "metadata": {
        "id": "-8l2uyRI00wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, shutil\n",
        "for p in glob.glob('/usr/local/lib/python*/dist-packages/~*'):\n",
        "    print(\"Removing\", p)\n",
        "    shutil.rmtree(p, ignore_errors=True)\n"
      ],
      "metadata": {
        "id": "DqOLGvBfspsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SB3-safe cv2 shim: run this BEFORE `import stable_baselines3 as sb3`\n",
        "import sys, types\n",
        "try:\n",
        "    import cv2  # Colab may auto-load a minimal cv2\n",
        "except Exception:\n",
        "    cv2 = None\n",
        "\n",
        "if cv2 is None:\n",
        "    cv2 = types.ModuleType(\"cv2\")\n",
        "    cv2.ocl = types.SimpleNamespace(setUseOpenCL=lambda *a, **k: None)\n",
        "    sys.modules[\"cv2\"] = cv2\n",
        "else:\n",
        "    if not hasattr(cv2, \"ocl\"):\n",
        "        cv2.ocl = types.SimpleNamespace(setUseOpenCL=lambda *a, **k: None)\n",
        "    elif not hasattr(cv2.ocl, \"setUseOpenCL\"):\n",
        "        cv2.ocl.setUseOpenCL = lambda *a, **k: None\n",
        "\n",
        "# Now safe to import SB3 & friends\n",
        "import torch, gymnasium, stable_baselines3 as sb3, transformers, pandas as pd, numpy as np\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| GPU:\", torch.cuda.is_available())\n",
        "print(\"Gymnasium:\", gymnasium.__version__)\n",
        "print(\"SB3:\", sb3.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "print(\"pandas:\", pd.__version__, \"| numpy:\", np.__version__)\n"
      ],
      "metadata": {
        "id": "h-lNJSkXstmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shimmy_needed = False\n",
        "try:\n",
        "    import gym_anytrading  # legacy gym-based\n",
        "    shimmy_needed = True\n",
        "    print(\"Detected gym-anytrading → Shimmy wrapper recommended.\")\n",
        "except Exception:\n",
        "    print(\"gym-anytrading not installed → Shimmy not needed.\")\n",
        "\n",
        "print(\"Shimmy needed? \", shimmy_needed)\n"
      ],
      "metadata": {
        "id": "hoCOU9Sas6_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gym-anytrading shimmy"
      ],
      "metadata": {
        "id": "UKPWP-Pps-oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Where you keep all PPO artifacts\n",
        "RESULTS_ROOT = \"/content/drive/MyDrive/Results_May_2025\"\n",
        "MASTER_DIR   = os.path.join(RESULTS_ROOT, \"ppo_models_master\")\n",
        "os.makedirs(MASTER_DIR, exist_ok=True)\n",
        "\n",
        "# Pick a symbol/prefix you’re uploading for (repeat for CVX, etc.)\n",
        "PREFIX = \"ppo_GE_window1\"   # change to \"ppo_CVX_window1\" when uploading CVX files\n",
        "\n",
        "# === 1) Upload PPO artifacts from your local machine ===\n",
        "# Expecting files named exactly like:\n",
        "#   ppo_GE_window1_model.zip\n",
        "#   ppo_GE_window1_vecnorm.pkl\n",
        "#   ppo_GE_window1_features.json\n",
        "# (optionally) ppo_GE_window1_probability_config.json, ppo_GE_window1_model_info.json\n",
        "uploaded = files.upload()   # choose the files above\n",
        "\n",
        "# Persist uploads to your MASTER_DIR\n",
        "for name, data in uploaded.items():\n",
        "    # Ensure file exists on the runtime filesystem (Colab sometimes does this automatically)\n",
        "    with open(name, \"wb\") as f:\n",
        "        f.write(data)\n",
        "    # Move into your master artifacts folder\n",
        "    shutil.move(name, os.path.join(MASTER_DIR, name))\n",
        "\n",
        "print(\"Saved to:\", MASTER_DIR)\n",
        "print(sorted([f for f in os.listdir(MASTER_DIR) if f.startswith(PREFIX)]))"
      ],
      "metadata": {
        "id": "Lbr47SbWC5HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Upload your local env file (e.g., Github.env.txt or .env.github)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()   # run this cell and choose the file"
      ],
      "metadata": {
        "id": "fK_jkdQlHB3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2) Rename to .env (only if your filename isn't already \".env\")\n",
        "import os\n",
        "if \"Github_key.env.txt\" in uploaded:\n",
        "    os.rename(\"Github_key.env.txt\", \".env\")    # adjust if your uploaded name differs\n"
      ],
      "metadata": {
        "id": "5We3sEjgpqhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, shutil\n",
        "\n",
        "RESULTS_ROOT = \"/content/drive/MyDrive/Results_May_2025\"\n",
        "MASTER_DIR   = os.path.join(RESULTS_ROOT, \"ppo_models_master\")\n",
        "os.makedirs(MASTER_DIR, exist_ok=True)\n",
        "\n",
        "PREFIX = \"ppo_GE_window1\"  # change to ppo_CVX_window1 for CVX\n",
        "\n",
        "# Only upload if the key files are missing\n",
        "needed = [f\"{PREFIX}_model.zip\", f\"{PREFIX}_vecnorm.pkl\", f\"{PREFIX}_features.json\"]\n",
        "missing = [f for f in needed if not os.path.exists(os.path.join(MASTER_DIR, f))]\n",
        "if missing:\n",
        "    from google.colab import files\n",
        "    print(\"Missing:\", missing, \"\\nPlease upload the listed files.\")\n",
        "    uploaded = files.upload()\n",
        "    for name, data in uploaded.items():\n",
        "        with open(name, \"wb\") as f:\n",
        "            f.write(data)\n",
        "        shutil.move(name, os.path.join(MASTER_DIR, name))\n",
        "    print(\"Saved to:\", MASTER_DIR)\n",
        "\n",
        "# Token: prefer .env OR set once in-session, not both\n",
        "token = os.getenv(\"GITHUB_TOKEN\")\n",
        "if not token:\n",
        "    try:\n",
        "        # If you keep secrets in a .env file on Drive:\n",
        "        # !pip -q install python-dotenv\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv(\".env\")\n",
        "        token = os.getenv(\"GITHUB_TOKEN\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not token:\n",
        "    # Last-resort secure prompt (doesn't echo)\n",
        "    from getpass import getpass\n",
        "    token = getpass(\"Paste your GitHub token (won't be printed): \").strip()\n",
        "    os.environ[\"GITHUB_TOKEN\"] = token\n",
        "\n",
        "print(\"Artifacts present:\", all(os.path.exists(os.path.join(MASTER_DIR, f)) for f in needed))\n",
        "print(\"Token present?   \", bool(os.getenv(\"GITHUB_TOKEN\")))\n"
      ],
      "metadata": {
        "id": "t-MxocSjHIiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------- 3) IMPORTS -------------------------------------------\n",
        "import os, json, time, logging\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import yfinance as yf\n",
        "\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# gym-anytrading (legacy Gym API); harmless banner may print on import\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "from gymnasium.spaces import Box as GBox\n",
        "\n",
        "# ---------------------------------- 4) LOGGING & COLAB DRIVE ---------------------------------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "L = logging.getLogger(\"producer\")\n",
        "\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    if not os.path.ismount(\"/content/drive\"):\n",
        "        drive.mount(\"/content/drive\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---------------------------------- 5) PATHS & CONFIG ----------------------------------------\n",
        "RESULTS_ROOT = \"/content/drive/MyDrive/Results_May_2025\"\n",
        "MASTER_DIR   = os.path.join(RESULTS_ROOT, \"ppo_models_master\")   # where <prefix>_model.zip etc. live\n",
        "os.makedirs(MASTER_DIR, exist_ok=True)\n",
        "\n",
        "STATE_DIR    = \"/content/drive/MyDrive/QuantConnect_Ready\"\n",
        "os.makedirs(STATE_DIR, exist_ok=True)\n",
        "GIST_ID_PATH = os.path.join(STATE_DIR, \"live_signals_gist_id.txt\")\n",
        "\n",
        "# Which models to publish\n",
        "PICKS: Dict[str, str] = {\n",
        "    \"GE\":  \"ppo_GE_window1\",\n",
        "    \"CVX\": \"ppo_CVX_window1\",    # leave in; will gracefully warn if artifacts missing\n",
        "}\n",
        "\n",
        "# yfinance fetch params\n",
        "YF_INTERVAL = \"1m\"   # 1-minute bars\n",
        "YF_DAYS     = 5      # yfinance supports about 7 days of 1m history\n",
        "\n",
        "# ---------------------------------- 6) AUTH / SECRETS ----------------------------------------\n",
        "# Load .env if present; otherwise you can set os.environ[\"GITHUB_TOKEN\"] = \"ghp_...\"\n",
        "if os.path.exists(\".env\"):\n",
        "    load_dotenv(\".env\")\n",
        "\n",
        "GITHUB_TOKEN  = os.environ.get(\"GITHUB_TOKEN\", \"\").strip()\n",
        "GIST_ID       = os.environ.get(\"GIST_ID\", \"\").strip()        # optional override; else persisted to file\n",
        "GIST_FILENAME = \"live_signals.json\"\n",
        "GIST_DESC     = \"Live PPO signals for QC (Producer→Consumer)\"\n",
        "\n",
        "# ---------------------------------- 7) OPTIONAL UPLOAD (INTERACTIVE) -------------------------\n",
        "# If you need to upload artifacts from your local machine, set this True and follow the prompt.\n",
        "ENABLE_INTERACTIVE_UPLOAD = False\n",
        "UPLOAD_PREFIX = \"ppo_GE_window1\"   # change to \"ppo_CVX_window1\" to upload CVX\n",
        "\n",
        "if ENABLE_INTERACTIVE_UPLOAD:\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "        needed = [f\"{UPLOAD_PREFIX}_model.zip\", f\"{UPLOAD_PREFIX}_vecnorm.pkl\", f\"{UPLOAD_PREFIX}_features.json\"]\n",
        "        missing = [f for f in needed if not os.path.exists(os.path.join(MASTER_DIR, f))]\n",
        "        if missing:\n",
        "            print(\"Missing files:\", missing)\n",
        "            print(\"Please upload the listed files (they will be moved into ppo_models_master).\")\n",
        "            uploaded = files.upload()\n",
        "            for name, data in uploaded.items():\n",
        "                with open(name, \"wb\") as f:\n",
        "                    f.write(data)\n",
        "                shutil.move(name, os.path.join(MASTER_DIR, name))\n",
        "            print(\"Saved to:\", MASTER_DIR)\n",
        "    except Exception as e:\n",
        "        print(\"Upload step skipped or failed:\", e)\n",
        "\n",
        "# ---------------------------------- 8) FEATURE PIPELINE (DUMMY) ------------------------------\n",
        "# Replace with your real feature computation if you have one.\n",
        "def compute_enhanced_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df_in\n",
        "\n",
        "# ---------------------------------- 9) TRADING ENV (StocksEnv → Gymnasium) -------------------\n",
        "class ContinuousPositionEnv(StocksEnv):\n",
        "    \"\"\"\n",
        "    Wraps gym_anytrading.envs.StocksEnv (legacy Gym) but returns Gymnasium-style (obs, reward, terminated, truncated, info).\n",
        "    Action: continuous position target in [-1, +1].\n",
        "    \"\"\"\n",
        "    def __init__(self, df, frame_bound, window_size,\n",
        "                 cost_rate=0.0002, slip_rate=0.0003,\n",
        "                 k_alpha=0.20, k_mom=0.05, k_sent=0.0,\n",
        "                 mom_source=\"denoised\", mom_lookback=20,\n",
        "                 min_trade_delta=0.01, cooldown=5, reward_clip=1.0):\n",
        "        super().__init__(df=df.reset_index(drop=True), frame_bound=frame_bound, window_size=window_size)\n",
        "        # Ensure spaces are Gymnasium Boxes\n",
        "        if isinstance(self.observation_space, gym.spaces.Box):\n",
        "            self.observation_space = GBox(\n",
        "                low=self.observation_space.low,\n",
        "                high=self.observation_space.high,\n",
        "                shape=self.observation_space.shape,\n",
        "                dtype=self.observation_space.dtype,\n",
        "            )\n",
        "        self.action_space = GBox(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "        self.cost_rate, self.slip_rate = float(cost_rate), float(slip_rate)\n",
        "        self.k_alpha, self.k_mom = float(k_alpha), float(k_mom)\n",
        "        self.k_sent = float(k_sent)\n",
        "        self.mom_source, self.mom_lookback = str(mom_source), int(mom_lookback)\n",
        "        self.min_trade_delta, self.cooldown = float(min_trade_delta), int(cooldown)\n",
        "        self.reward_clip = float(reward_clip)\n",
        "\n",
        "        self.nav, self.pos, self._last_trade_step = 1.0, 0.0, -self.cooldown\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        out = super().reset(**kwargs)\n",
        "        obs, info = (out if isinstance(out, tuple) else (out, {}))\n",
        "        self.nav, self.pos, self._last_trade_step = 1.0, 0.0, -self.cooldown\n",
        "        info = info or {}\n",
        "        info.update({\"nav\": self.nav, \"pos\": self.pos})\n",
        "        return obs, info\n",
        "\n",
        "    def _step_parent_hold(self):\n",
        "        # Parent env uses discrete actions; action 2 = HOLD (no-op)\n",
        "        step_result = super().step(2)\n",
        "        if len(step_result) == 5:  # already Gymnasium format\n",
        "            obs, _env_rew, terminated, truncated, info = step_result\n",
        "        else:                      # legacy Gym 4-tuple\n",
        "            obs, _env_rew, done, info = step_result\n",
        "            terminated, truncated = bool(done), False\n",
        "        return obs, terminated, truncated, info\n",
        "\n",
        "    def _ret_t(self):\n",
        "        cur  = float(self.df.loc[self._current_tick, 'Close'])\n",
        "        prev = float(self.df.loc[max(self._current_tick - 1, 0), 'Close'])\n",
        "        return 0.0 if prev <= 0 else (cur - prev) / prev\n",
        "\n",
        "    def _mom_signal(self):\n",
        "        if self.mom_source == \"macd\" and \"MACD_Line\" in self.df.columns:\n",
        "            recent = self.df[\"MACD_Line\"].iloc[max(self._current_tick-200,0):self._current_tick+1]\n",
        "            return float(np.tanh(float(self.df.loc[self._current_tick, \"MACD_Line\"]) / (1e-6 + recent.std())))\n",
        "        if \"Denoised_Close\" in self.df.columns and self._current_tick - self.mom_lookback >= 0:\n",
        "            now  = float(self.df.loc[self._current_tick, \"Denoised_Close\"])\n",
        "            then = float(self.df.loc[self._current_tick - self.mom_lookback, \"Denoised_Close\"])\n",
        "            base = float(self.df.loc[max(self._current_tick - 1, 0), \"Close\"])\n",
        "            slope = (now - then) / max(self.mom_lookback, 1)\n",
        "            return float(np.tanh(10.0 * (slope / max(abs(base), 1e-6))))\n",
        "        return 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        a = float(np.array(action).squeeze())\n",
        "        target_pos = float(np.clip(a, -1.0, 1.0))\n",
        "\n",
        "        r_t = self._ret_t()\n",
        "        base_ret = self.pos * r_t\n",
        "\n",
        "        changed = (abs(target_pos - self.pos) >= self.min_trade_delta) and \\\n",
        "                  ((self._current_tick - self._last_trade_step) >= self.cooldown)\n",
        "        delta_pos = (target_pos - self.pos) if changed else 0.0\n",
        "        trade_cost = (self.cost_rate + self.slip_rate) * abs(delta_pos)\n",
        "\n",
        "        rel_alpha  = base_ret - r_t\n",
        "        mom_term   = self.pos * self._mom_signal()\n",
        "        shaped     = base_ret + self.k_alpha*rel_alpha + self.k_mom*mom_term - trade_cost\n",
        "        reward     = float(np.clip(shaped, -self.reward_clip, self.reward_clip))\n",
        "\n",
        "        self.nav  *= (1.0 + base_ret - trade_cost)\n",
        "        if changed:\n",
        "            self.pos = target_pos\n",
        "            self._last_trade_step = self._current_tick\n",
        "\n",
        "        obs, terminated, truncated, info = self._step_parent_hold()\n",
        "        info = info or {}\n",
        "        info.update({\"ret_t\": r_t, \"nav\": self.nav, \"pos\": self.pos,\n",
        "                     \"trade_cost\": trade_cost, \"base_ret\": base_ret,\n",
        "                     \"rel_alpha\": rel_alpha, \"mom\": self._mom_signal()})\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "# ------------------------------- 10) ARTIFACT HELPERS ----------------------------------------\n",
        "def _features_list_for(prefix: str) -> List[str]:\n",
        "    fpath = os.path.join(MASTER_DIR, f\"{prefix}_features.json\")\n",
        "    if os.path.exists(fpath):\n",
        "        try:\n",
        "            meta = json.load(open(fpath, \"r\"))\n",
        "            feats = meta.get(\"features\") or []\n",
        "            if isinstance(feats, list):\n",
        "                return [str(c) for c in feats]\n",
        "        except Exception as e:\n",
        "            L.warning(f\"features.json read failed for {prefix}: {e}\")\n",
        "    return []\n",
        "\n",
        "def _align_columns(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
        "    feats = _features_list_for(prefix)\n",
        "    if not feats:\n",
        "        return df\n",
        "    aligned = df.copy()\n",
        "    for c in feats:\n",
        "        if c not in aligned.columns:\n",
        "            aligned[c] = 0.0\n",
        "    ordered = [c for c in feats if c in aligned.columns] + [c for c in aligned.columns if c not in feats]\n",
        "    return aligned[ordered]\n",
        "\n",
        "def _check_artifacts(prefix: str) -> Dict[str, bool]:\n",
        "    need = [\"_model.zip\", \"_vecnorm.pkl\"]\n",
        "    nice = [\"_features.json\", \"_probability_config.json\", \"_model_info.json\"]\n",
        "    return {s: os.path.exists(os.path.join(MASTER_DIR, prefix + s)) for s in need + nice}\n",
        "\n",
        "# -------------------------------- 11) MODEL / ENV LOADING ------------------------------------\n",
        "def get_mu_sigma(model, obs):\n",
        "    with torch.no_grad():\n",
        "        obs_t, _     = model.policy.obs_to_tensor(obs)\n",
        "        feats        = model.policy.extract_features(obs_t)\n",
        "        latent_pi, _ = model.policy.mlp_extractor(feats)\n",
        "        mean_actions = model.policy.action_net(latent_pi)\n",
        "        log_std      = model.policy.log_std\n",
        "        mu    = float(mean_actions.detach().cpu().numpy().squeeze())\n",
        "        sigma = float(log_std.exp().detach().cpu().numpy().squeeze())\n",
        "    return mu, sigma\n",
        "\n",
        "def load_model_and_env(prefix: str):\n",
        "    model_path = os.path.join(MASTER_DIR, f\"{prefix}_model.zip\")\n",
        "    vec_path   = os.path.join(MASTER_DIR, f\"{prefix}_vecnorm.pkl\")\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Missing model: {model_path}\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = PPO.load(model_path, device=device)\n",
        "\n",
        "    def make_env(df_window: pd.DataFrame):\n",
        "        # simple frame bound near the end\n",
        "        frame_bound = (max(50, len(df_window)//3), len(df_window) - 3)\n",
        "        e = DummyVecEnv([lambda: ContinuousPositionEnv(\n",
        "            df=df_window, frame_bound=frame_bound, window_size=10,\n",
        "            cost_rate=0.0002, slip_rate=0.0003,\n",
        "            k_alpha=0.20, k_mom=0.05, k_sent=0.0,\n",
        "            mom_source=\"denoised\", mom_lookback=20,\n",
        "            min_trade_delta=0.01, cooldown=5, reward_clip=1.0\n",
        "        )])\n",
        "        if os.path.exists(vec_path):\n",
        "            e = VecNormalize.load(vec_path, e)\n",
        "        e.training = False\n",
        "        e.norm_reward = False\n",
        "        return e\n",
        "    return model, make_env\n",
        "\n",
        "# -------------------------------- 12) DATA FETCH / PREP --------------------------------------\n",
        "def _flatten_yf_columns(df: pd.DataFrame, symbol: str) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        new_cols = []\n",
        "        for c0, c1 in df.columns.to_list():\n",
        "            if c1 in (\"\", symbol):\n",
        "                new_cols.append(c0)\n",
        "            else:\n",
        "                new_cols.append(f\"{c0}_{c1}\")\n",
        "        df.columns = new_cols\n",
        "    return df\n",
        "\n",
        "def latest_df_for_symbol(symbol: str, horizon_days: int = YF_DAYS, interval: str = YF_INTERVAL) -> pd.DataFrame | None:\n",
        "    end   = datetime.now(timezone.utc)\n",
        "    start = end - timedelta(days=horizon_days)\n",
        "    df = yf.download(symbol,\n",
        "                     start=start.strftime(\"%Y-%m-%d\"),\n",
        "                     end=end.strftime(\"%Y-%m-%d\"),\n",
        "                     interval=interval,\n",
        "                     progress=False,\n",
        "                     auto_adjust=False)\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "    df = df.reset_index()\n",
        "    df[\"Symbol\"] = symbol\n",
        "    df = _flatten_yf_columns(df, symbol)\n",
        "    df = compute_enhanced_features(df)\n",
        "    return df\n",
        "\n",
        "# -------------------------------- 13) INFERENCE ----------------------------------------------\n",
        "def predict_latest(symbol: str, prefix: str) -> Dict[str, Any]:\n",
        "    status = _check_artifacts(prefix)\n",
        "    missing_hard = [k for k in [\"_model.zip\", \"_vecnorm.pkl\"] if not status.get(k, False)]\n",
        "    if missing_hard:\n",
        "        return {\"symbol\": symbol, \"prefix\": prefix, \"error\": f\"missing artifacts: {missing_hard}\"}\n",
        "\n",
        "    model, make_env = load_model_and_env(prefix)\n",
        "    live_df = latest_df_for_symbol(symbol)\n",
        "    if live_df is None or len(live_df) < 120:\n",
        "        return {\"symbol\": symbol, \"prefix\": prefix, \"error\": \"no fresh data\"}\n",
        "\n",
        "    live_df = _align_columns(live_df, prefix)\n",
        "    df_window = live_df.iloc[-2500:].reset_index(drop=True) if len(live_df) > 2500 else live_df.copy()\n",
        "\n",
        "    env = make_env(df_window)\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple):\n",
        "        obs, _ = obs\n",
        "\n",
        "    # Roll forward with a neutral policy to get to the end-of-window observation\n",
        "    for _ in range(max(1, len(df_window) - 5)):\n",
        "        obs, _, dones, _ = env.step([np.array([0.0], dtype=np.float32)])  # (n_envs, action_dim)\n",
        "        if isinstance(dones, (np.ndarray, list)) and bool(dones[0]):\n",
        "            break\n",
        "\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    mu, sigma = get_mu_sigma(model, obs)\n",
        "    from math import erf, sqrt\n",
        "    Phi   = lambda x: 0.5 * (1.0 + erf(x / sqrt(2.0)))\n",
        "    p_long = 1.0 - Phi((0.0 - mu) / max(sigma, 1e-6))\n",
        "\n",
        "    a = float(np.array(action).squeeze())\n",
        "    signal = \"BUY\" if a > 0.10 else (\"SELL\" if a < -0.30 else \"HOLD\")\n",
        "    ts = df_window[\"Datetime\"].iloc[-1] if \"Datetime\" in df_window.columns else None\n",
        "    px = float(df_window[\"Close\"].iloc[-1])\n",
        "\n",
        "    return {\n",
        "        \"symbol\": symbol,\n",
        "        \"prefix\": prefix,\n",
        "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"bar_ts\": str(ts),\n",
        "        \"price\": px,\n",
        "        \"action\": a,\n",
        "        \"signal\": signal,\n",
        "        \"confidence\": abs(a),\n",
        "        \"p_long\": float(p_long),\n",
        "        \"p_short\": float(1.0 - p_long),\n",
        "        \"mu\": float(mu),\n",
        "        \"sigma\": float(sigma)\n",
        "    }\n",
        "\n",
        "# -------------------------------- 14) GIST HELPERS (ROBUST) ----------------------------------\n",
        "def _headers(token: str) -> Dict[str, str]:\n",
        "    return {\"Authorization\": f\"token {token}\"} if token else {}\n",
        "\n",
        "def _load_saved_gist_id() -> str:\n",
        "    if GIST_ID:\n",
        "        return GIST_ID\n",
        "    try:\n",
        "        gid = open(GIST_ID_PATH, \"r\").read().strip()\n",
        "        return gid if gid else \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def _save_gist_id(gid: str):\n",
        "    try:\n",
        "        with open(GIST_ID_PATH, \"w\") as f:\n",
        "            f.write(gid)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def publish_json_to_gist(payload: dict, filename: str, gist_id: str, token: str, desc: str) -> dict:\n",
        "    \"\"\"\n",
        "    Returns: {\"id\": <gist_id>, \"owner\": <login or 'anonymous'>, \"raw_url\": <raw file url or ''>}\n",
        "    \"\"\"\n",
        "    if not token:\n",
        "        raise RuntimeError(\"GITHUB_TOKEN not set. Set os.environ['GITHUB_TOKEN']='ghp_...' or create a .env first.\")\n",
        "\n",
        "    files = {filename: {\"content\": json.dumps(payload, indent=2)}}\n",
        "\n",
        "    if gist_id:\n",
        "        r = requests.patch(f\"https://api.github.com/gists/{gist_id}\",\n",
        "                           headers=_headers(token),\n",
        "                           json={\"files\": files, \"description\": desc}, timeout=30)\n",
        "    else:\n",
        "        r = requests.post(\"https://api.github.com/gists\",\n",
        "                          headers=_headers(token),\n",
        "                          json={\"files\": files, \"description\": desc, \"public\": True},\n",
        "                          timeout=30)\n",
        "\n",
        "    if not r.ok:\n",
        "        raise RuntimeError(f\"Gist API error {r.status_code}: {r.text[:300]}\")\n",
        "\n",
        "    data = r.json()\n",
        "    gid = data.get(\"id\", gist_id or \"\")\n",
        "    owner = ((data.get(\"owner\") or {}).get(\"login\")) or \"anonymous\"\n",
        "    raw_url = ((data.get(\"files\") or {}).get(filename) or {}).get(\"raw_url\", \"\")\n",
        "\n",
        "    if not gid:\n",
        "        raise RuntimeError(\"Gist ID missing in API response.\")\n",
        "\n",
        "    if not raw_url:\n",
        "        m = requests.get(f\"https://api.github.com/gists/{gid}\", headers=_headers(token), timeout=30)\n",
        "        if m.ok and m.headers.get(\"content-type\",\"\").startswith(\"application/json\"):\n",
        "            md = m.json()\n",
        "            raw_url = ((md.get(\"files\") or {}).get(filename) or {}).get(\"raw_url\", \"\")\n",
        "\n",
        "    _save_gist_id(gid)\n",
        "    return {\"id\": gid, \"owner\": owner, \"raw_url\": raw_url}\n",
        "\n",
        "def gist_raw_url(gist_id: str, filename: str, token: str = \"\") -> str:\n",
        "    if not gist_id:\n",
        "        return \"\"\n",
        "    r = requests.get(f\"https://api.github.com/gists/{gist_id}\", headers=_headers(token), timeout=20)\n",
        "    if not r.ok or not r.headers.get(\"content-type\",\"\").startswith(\"application/json\"):\n",
        "        return \"\"\n",
        "    data = r.json()\n",
        "    return ((data.get(\"files\") or {}).get(filename) or {}).get(\"raw_url\", \"\") or \"\"\n",
        "\n",
        "# -------------------------------- 15) PUBLISH LOOP -------------------------------------------\n",
        "RUN_LOOP  = False\n",
        "SLEEP_SEC = 60\n",
        "\n",
        "def run_once():\n",
        "    # Optionally filter out symbols missing core artifacts so the Gist shows only valid models\n",
        "    results = []\n",
        "    for sym, pref in PICKS.items():\n",
        "        try:\n",
        "            out = predict_latest(sym, pref)\n",
        "            if out.get(\"error\"):\n",
        "                L.warning(f\"{sym} -> {out['error']}\")\n",
        "            results.append(out)\n",
        "        except Exception as e:\n",
        "            L.exception(f\"{sym} predict error: {e}\")\n",
        "            results.append({\"symbol\": sym, \"prefix\": pref, \"error\": str(e)})\n",
        "\n",
        "    payload = {\n",
        "        \"generated_utc\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"valid_until_utc\": (datetime.now(timezone.utc) + timedelta(minutes=3)).isoformat(),\n",
        "        \"producer\": \"colab-sb3\",\n",
        "        \"interval\": YF_INTERVAL,\n",
        "        \"models\": results\n",
        "    }\n",
        "\n",
        "    meta = publish_json_to_gist(payload, filename=GIST_FILENAME,\n",
        "                                gist_id=_load_saved_gist_id(),\n",
        "                                token=GITHUB_TOKEN, desc=GIST_DESC)\n",
        "\n",
        "    gid = meta[\"id\"]\n",
        "    raw = meta.get(\"raw_url\") or gist_raw_url(gid, GIST_FILENAME, token=GITHUB_TOKEN)\n",
        "\n",
        "    print(\"Published:\", f\"https://gist.github.com/{gid}\")\n",
        "    print(\"RAW URL  :\", raw or \"(raw url not available yet — open gist page)\")\n",
        "    print(\"Preview  :\", json.dumps(payload, indent=2)[:900], \"...\")\n",
        "\n",
        "# -------------------------------- 16) SANITY CHECK + EXECUTE ---------------------------------\n",
        "for sym, pref in PICKS.items():\n",
        "    status = _check_artifacts(pref)\n",
        "    hard_miss = [k for k in [\"_model.zip\", \"_vecnorm.pkl\"] if not status.get(k, False)]\n",
        "    print(f\"{pref}: {'OK' if not hard_miss else 'MISSING ' + str(hard_miss)}\")\n",
        "\n",
        "# Fallback prompt if token missing (won't echo); comment out if you prefer exception instead\n",
        "if not GITHUB_TOKEN:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        os.environ[\"GITHUB_TOKEN\"] = getpass(\"Paste your GitHub token (won't be printed): \").strip()\n",
        "        GITHUB_TOKEN = os.environ[\"GITHUB_TOKEN\"]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "run_once()\n",
        "\n",
        "if RUN_LOOP:\n",
        "    while True:\n",
        "        try:\n",
        "            run_once()\n",
        "        except Exception as e:\n",
        "            L.error(f\"Publish error: {e}\")\n",
        "        time.sleep(SLEEP_SEC)\n",
        "\n",
        "# -------------------------------- 17) OPTIONAL: PRINT STABLE RAW URL LATER -------------------\n",
        "try:\n",
        "    gid = open(GIST_ID_PATH, \"r\").read().strip()\n",
        "    if gid:\n",
        "        r = requests.get(f\"https://api.github.com/gists/{gid}\", headers=_headers(GITHUB_TOKEN), timeout=20)\n",
        "        if r.ok and r.headers.get(\"content-type\",\"\").startswith(\"application/json\"):\n",
        "            meta = r.json()\n",
        "            owner = (meta.get(\"owner\") or {}).get(\"login\", \"anonymous\")\n",
        "            raw_url_stable = f\"https://gist.githubusercontent.com/{owner}/{gid}/raw/{GIST_FILENAME}\"\n",
        "            raw_url_api    = (meta.get(\"files\", {}).get(GIST_FILENAME, {}) or {}).get(\"raw_url\", \"\")\n",
        "            print(\"Gist page   :\", f\"https://gist.github.com/{owner}/{gid}\")\n",
        "            print(\"Raw (stable):\", raw_url_stable)\n",
        "            print(\"Raw (API)   :\", raw_url_api)\n",
        "        else:\n",
        "            print(\"Gist meta fetch skipped (non-JSON or HTTP error).\")\n",
        "    else:\n",
        "        print(\"No saved Gist ID yet.\")\n",
        "except Exception as e:\n",
        "    print(\"Gist meta check skipped:\", e)"
      ],
      "metadata": {
        "id": "6cMq8kbmgzts"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}